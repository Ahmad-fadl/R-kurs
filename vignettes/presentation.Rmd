---
title: "R Course Project"
subtitle: "Clustering"
output: ioslides_presentation
vignette: >
  %\VignetteIndexEntry{presentation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE}
library(clustering)
```

# Introduction
In this project, we implement various clustering algorithms such as K-Means, K-Medoids, Hierarchical Clustering, Spectral clustering, and density based clustering algorithms such as DBSCAN, and OPTICS, and finally put them together to form a R package.

## The Goal of Clustering

- given a set of input data
- imput data is grouped together to form clusters where similar objects are grouped together
- try to identify related vectors based on distance metric
- groups of related vectors are called a ***cluster***

```{r, echo=FALSE, fig.asp=.55, warning=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

set.seed(188)
n <- 50
x <- cbind(
  x = runif(5, 0, 2) + rnorm(n, sd = 0.05),
  y = runif(5, 0, 2) + rnorm(n, sd = 0.05)
)
x <- t(x)
d <- c(x, .6, .75, .8, 1.3, 0, 1.6, .6, 1, .9, .9, .5, 1.5, 1, .4, .1, .6);
x <- matrix(d, nrow=2);

y <- x
attr(y, "cluster") <- rep(-1, n)
clustering::plot_clustered_2d_data(y,show_noise = TRUE);
clustering::plot_clustered_2d_data(DBSCAN(x, .1, 7));
```

## Input Data

- data is saved in matrices
- columns are vectors
- $\text{data} \subset \mathbb{R}^{\text{nrow(data)}}$
- $\#\text{data} = ncol(data)$

```{r, echo=FALSE, fig.cap="Sample data: Five two-dimensional Vectors"}
print(generate_2d_cluster(5))
```

## Output Data

- algorithms return input data
- cluster information saved in the attribute `"cluster"`
- non-positive cluster values denote noise, e.g. `-1`

```{r, echo=FALSE}
X <- generate_2d_cluster(5)
attr(X, "cluster") <- c(1,-1,2,2,1)
print(X)
```


# Clustering Algorithm Example

## K-Medoids Clustering

- used to partition a data set into k clusters
- medoid = point with minimum dissimilarities to other points in cluster
- swaps medoids to reach minimal cost

```{r, fig.asp=.35, cache=TRUE, echo=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

X <- matrix(c(8,7,3,7,4,9,9,6,8,5,5,8,7,3,8,4,7,5,4,5), nrow = 2)
plot(X[1,], X[2,], xlab="", ylab="")
title("Initial Data")

data <- clustering::k_medoids(X, 2)
plot_clustered_2d_data(data)
title("Resulting Clustering (k = 2)")
```
## K-Means Clustering
-is a method of groubing data
- used to partition a data set into k clusters
- each observation belongs to the cluster with the nearest mean


```{r, fig.asp=.35, cache=TRUE, echo=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

x = rep(1:10)
x= matrix(sample(x,10),nrow = 2)
plot(X[1,], X[2,], xlab="", ylab="")
title("Initial Data")

mydata = clustering::kmean(X, 2)
plot_clustered_2d_data(mydata)
title("Resulting Clustering (k = 2)")
```

## Spectral Clustering

- generates projections of input vectors
- does not directly produce a clustering
- by running another algorithm on the projections clusters can be identified

<!-- Note: - colors are not visible to the algorithm -> only for visualization purposes -->
<!--       - k-medoids algorithm used to find clusters -->


```{r, fig.asp=.35, cache=TRUE, echo=FALSE}
par(mfrow=c(1,3), mar=c(3,3,2,1))

set.seed(1)

X <- clustering::generate_nested_2d_training_data(400)
plot(X[1,], X[2,], xlab="", ylab="", col=c("darkred", "darkblue")[attr(X, "innerOrOuter")])
title("Nested Clusters")

data <- clustering::spectral_clustering(X, k=2)
plot(data[1,], data[2,], col=c("darkred", "darkblue")[attr(X, "innerOrOuter")], xlab="", ylab="", main="Generated Projections")

cData <- clustering::spectral_clustering(X, k=2, mercer_kernel=gaussian_kernel_with_fixed_gamma(30), clustering=k_medoids, 2);
plot_clustered_2d_data(cData)
title("Resulting Clustering")
```


# The Package

<!-- 3. Paket vorstellen, -->
<!-- Was erwarten die Clustering Funktionen f체r Argumente -->
<!-- Was geben die f체r Ausgaben -->
<!-- Liste aller Funktionen -->
<!-- Darauf eingehen, das die alle ungef채hr dieselben Argumente brauchen -->

## Package Overview

- all algorithms expect the input data as their first argument
- k-means and k-medoids get passed the number of clusters k
- density-based algorithms (DBSCAN, OPTICS) get passed $\varepsilon$ and minimum-points values
- spectral clustering may be passed a dimension to project into
- algorithms working with distances may be passed a custom metric
- hierarchical clustering method takes input data as matrix and distance linkage method as input and returns clusters, height, order, labels.

<!-- !! TODO: add your algorithms signature !!-->

```{r, eval=FALSE}
k_medoids(data, k = 2)
kmean(data, k = 2, distanceFunction = euclidean_distance_squared)
DBSCAN(data, eps, minPts, distanceFunction = euclidean_distance)
OPTICS(data, eps, minPts, distanceFunction = euclidean_distance)
spectral_clustering(data, k = 1, mercer_kernel)
hierclust(data, method=c("single","complete","average","median"))
```


# Live Demonstration

<!-- 4. Live-Beispiele -->
<!-- nur mit der Standardfunktionalit채t -->
<!-- (zu 4.: Fehlerausgaben funktionieren usw) -->
<!-- (zu 4.: Dokumentation) -->

<!-- TODO: everyones algorithm -->
<!-- probably no slides necessary as this is supposed to be a live demonstration -->
<!--  > maybe just empty title slides -->



# Special Functionality
## OPTICS
- OPTICS algorithm produces reachability data and an ordering
<!-- - reachability data is always one-dimensional -->
<!--=> its generated using metrics -->
- using the reachability data the clusters can be visualized
- DBSCAN clustering can be extracted from OPTICS data
- ordering can be used to connect points in the cluster

```{r, echo=FALSE}
set.seed(188)
n <- 50
x <- cbind(
  x = runif(5, 0, 2) + rnorm(n, sd = 0.05),
  y = runif(5, 0, 2) + rnorm(n, sd = 0.05)
)
x <- t(x)
d <- c(x, .6, .75, .8, 1.3, 0, 1.6, .6, 1, .9, .9, .5, 1.5, 1, .4, .1, .6);
x <- matrix(d, nrow=2);
```

```{r, fig.asp=.55, echo=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

par(ann=FALSE)
clustering::reachability_plot(clustering::OPTICS(x, .1, 7), .074)
title(main = "Reachability Plot", xlab = "")

plot_clustered_2d_data(extract_DBSCAN_clustering(clustering::OPTICS(x, .1, 7)), connect_to_predecessor=TRUE, point_size = .5)
title(main = "Extracted DBSCAN", xlab = "")
```

<!-- 5. Besonderen Funktionen -->
<!-- (Hierarchical Clustering: Dendrogramm, Voronoi-Zellen bei K-means, Optics die extra-Graphen) -->
## Hierarchical Clustering

- It is Agglomerative clustering.
- It is a bottom-up manner grouping of objects into clusters.
- Output is a tree and can be plot as dendogram.
- In this clustering algorithm, each object is initially considered as a single-element cluster (leaf).
- At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes).
- This procedure is iterated until all points are member of just one single big cluster (root).
- The result is a tree which can be plotted as a dendrogram.
- The distance metric: single, complete, average, median can used to to group objects into clusters.

## Real Data Example: data(USArrests and Iris) with different cluster agglomeration methods

```{r}
data <- distance(USArrests)
data_clust <- hierclust(data, method="single")
data_clust_complete <- hierclust(data, method = "complete")
data_clust_avg <- hierclust(data, method = "average")
plot(data_clust)
plot(data_clust_complete)
plot(data_clust_avg)
clusters_iris <- hierclust(distance(iris[, 3:4]))
plot(clusters_iris)
```

# Examples using Real World Data

# The End
