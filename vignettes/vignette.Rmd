---
title: "13 - Clustering: Functionality and Usage"
output: pdf_document
#output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{13 - Clustering}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(clustering)
```

# Clustering Algorithms

The package contains implementations of the following algorithms:

* Spectral Clustering
* DBSCAN
* OPTICS
* k-Mediods
* k-Means
* Hierarchical Clustering

In this vignette we demonstrate how these functions are used, the resulting data, and additional functionality of the package. A case study demonstrating the clustering methods themselves can be found in a separate document.

# General Structure of Input and Output Data

Every function in this package expects/uses matrices whose columns are the input/output vectors, e.g. a collection of two-dimensional vectors is stored in a matrix `m` with `nrow(m)=2`.

```{r, echo=FALSE}
print(generate_2d_cluster(5))
```
Clusters produced by the clustering algorithms will be stored in the `"cluster"` attribute of the output data. A negative value always denotes noise.
```{r, echo=FALSE}
X <- generate_2d_cluster(5)
attr(X, "cluster") <- c(1,-1,2,2,1)
print(X)
```

# Math Functions

```{r, eval=FALSE}
euclidean_norm(vector)
infinity_norm(vector)
one_norm(vector)
p_norm(vector, p)
euclidean_distance(vector1, vector2)
gaussian_kernel(vector1, vector2, gamma = 7.5)
```

The package offers a multitude of predefined math functions, many of them norms, and also function factories for easy generation of additional math functions, e.g. using the `induced_metric_factory` function factory the induced metric of any norm may be produced.

```{r, eval=FALSE}
gaussian_kernel_with_fixed_gamma(gamma)
p_norm_factory(p)
induced_metric_factory(vector_norm)
```

Should the user want to cluster data using a specific p-Norm, the `p_norm_factory` function factory can return a p-Norm function with a fixed p, the same is true for the Gaussian kernel's gamma value when using the `gaussian_kernel_with_fixed_gamma` function factory.

# Helper Functions for Demonstration and Testing Purposes

The package exports a few functions to generate and plot two-dimensional clusters which may be used to familiarize oneself with the functionality of and the difference between the clustering methods.

```{r, eval=FALSE}
generate_2d_cluster(n, center = c(0, 0))
```
Generates a single cluster of `n` vectors in a radius of 1 around a given `center` point.

```{r, eval=FALSE}
generate_nested_2d_training_data(n)
```
Generates two nested clusters of `n` vectors. In addition, the returned data contains an extra attribute specifying to which cluster a given vector belongs. This is not used by any clustering algorithm but may be used to color data in a plot or to check the quality of a clustering algorithms result.

The following figure demonstrates how to use the latter function:

```{r, fig.width=4, fig.height=4}
data <- generate_nested_2d_training_data(400)
plot(data[1,], data[2,],
     col=c("darkred", "darkblue")[attr(data, "innerOrOuter")],
     xlab="", ylab="")
```

```{r, eval=FALSE}
plot_clustered_2d_data(data, point_size = .75,
                       show_noise = TRUE,
                       show_legend = FALSE,
                       hide_axis_text = FALSE)
```
This function may be used to conveniently plot data clustered by this package's clustering algorithms. Only the first two dimensions are plotted, extra dimensions are ignored.


# Spectral Clustering

```{r, eval=FALSE}
spectral_clustering(data, k = 1, mercer_kernel = gaussian_kernel, clustering = NULL, ...)
```
The spectral clustering algorithm produces projections of the input data into lower dimensions, by passing a `k` value the target dimension may be specified. Should the given `k` be larger than the input's dimension the function will report an error. The Mercer-Kernel used during the generation of the output projections may also be customized.
Since the spectral clustering algorithm itself only projects the input data, a second algorithm has to be employed. This can either be done manually, using the data returned by the `spectral_clustering` function or by passing a clustering algorithm as the `clustering` argument. Any further arguments will be passed into the given clustering algorithm. If the `clustering` argument is set, the function will return clustered data.

```{r, eval=FALSE}
data <- generate_nested_2d_training_data(400)
projections <- spectral_clustering(data, k=2)
clustered_data <- spectral_clustering(X, k=2, 
                                      mercer_kernel=gaussian_kernel_with_fixed_gamma(30),
                                      clustering=k_medoids, 2)
```

```{r, fig.asp=.55, echo=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

set.seed(1)

X <- clustering::generate_nested_2d_training_data(400)
data <- clustering::spectral_clustering(X, k=2)
plot(data[1,], data[2,], col=c("darkred", "darkblue")[attr(X, "innerOrOuter")], xlab="", ylab="", main="Projections for gamma = 30")

cData <- clustering::spectral_clustering(X, k=2, mercer_kernel=gaussian_kernel_with_fixed_gamma(30), clustering=k_medoids, 2);
plot_clustered_2d_data(cData)
title(main="Clustered Using k-Mediods")
```

# DBSCAN and OPTICS

```{r, eval=FALSE}
DBSCAN(data, eps, minPts, distanceFunction = euclidean_distance)
OPTICS(data, eps, minPts, distanceFunction = euclidean_distance, threshold = NULL)
```

The DBSCAN and OPTICS functions both take the input data as their first argument, an epsilon value to determine the neighborhood of points as the second argument, and a minimum-points value used to determine core points as their third argument. The function use to calculate distance may be specified as the fourth argument.

```{r, eval=FALSE}
reachability_plot(data, threshold=NULL)
cluster_by_reachability(data, threshold)
```

While the DBSCAN returns the data with clustering information added, the OPTICS algorithm does not produce a clustering by default. It returns the input data with distance information added in the `"core-distance"` and `"reachability-distance"` attributes. The `reachability_plot` function can be used to visualize the reachability information, while the `cluster_by_reachability` function can be used to extract clusters via a threshold. By passing a threshold to the `reachability_plot` function this can also be visualized.

```{r, echo=FALSE}
set.seed(188)
n <- 50
x <- cbind(
  x = runif(5, 0, 2) + rnorm(n, sd = 0.05),
  y = runif(5, 0, 2) + rnorm(n, sd = 0.05)
)
x <- t(x)
d <- c(x, .6, .75, .8, 1.3, 0, 1.6, .6, 1, .9, .9, .5, 1.5, 1, .4, .1, .6);
x <- matrix(d, nrow=2);
```

```{r, eval=FALSE}
reachability_plot(OPTICS(data, .1, 7), .074)
extract_DBSCAN_clustering(OPTICS(data, .1, 7))
```

The left figure shows an example of such a reachability plot with a given threshold drawn.
Using the reachability and core distance information produced by the OPTICS algorithm, the DBSCAN clustering can be extracted. In addition, the OPTICS algorithm saves the predecessor of each point which allows for a more detailed plot than the DBSCAN itself. An example of this is shown in the right figure.


```{r, fig.asp=.55, echo=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

par(ann=FALSE)
clustering::reachability_plot(clustering::OPTICS(x, .1, 7), .074)
title(main = "Reachability Plot", xlab = "")

plot_clustered_2d_data(extract_DBSCAN_clustering(clustering::OPTICS(x, .1, 7)), connect_to_predecessor=TRUE, point_size = .35)
title(main = "Extracted DBSCAN", xlab = "")
```

# K-Medoids Algorithm

The k-medoids algorithm is a clustering approach related to k-means clustering. It is used to partition a data set into k clusters using medoids. The k-medoids algorithm requires the user to specify the number of clusters k.

```{r}
data <- matrix(c(8,7,3,7,4,9,9,6,8,5,5,8,7,3,8,4,7,5,4,5), nrow = 2)
k_medoids(data, 2)
```
# K-Means Algorithm

The k-means algorithm is a clustering way tha used standards mathematics distances to groub data. It is used to partition a data set into k clusters using means. The k-means algorithm requires the user to specify the number of clusters k.

```{r}
data <- matrix(c(8,7,3,7,4,9,9,6,8,5,5,8,7,3,8,4,7,5,4,5), nrow = 2)
kmean(data, 2)
```

# Hierarchical Clustering

Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other. The one kind of Hierarchical clustering is Agglomerative clustering which is a bottom-up manner grouping of objects into clusters. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.

```{r}
data <- distance(USArrests)
data_clust <- hierclust(data, method="single")
```
