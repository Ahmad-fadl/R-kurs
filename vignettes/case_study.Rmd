---
title: "13 - Clustering"
#output: rmarkdown::html_vignette
output:
  pdf_document:
    fig_crop: yes
vignette: >
  %\VignetteIndexEntry{case_study}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  #fig.height = 2,
  #fig.asp=1,
  fig.align = "center",
  out.extra='keepaspectratio',
  out.width ="\\textwidth"
)
```

```{r setup}
library(clustering)
```

```{r plot_helper, include=FALSE, cache=TRUE}
plot2d <- function (data, title="Untitled", xlabel=NULL, ylabel=NULL) {
  plot(data[1,], data[2,], cex.axis=.75, pch=1, cex=.5, col=rgb(0, 0, 0, .5), main=title, xlab="", ylab="");
  if (!missing(xlabel)) mtext(side=1, line=2, xlabel);
  if (!missing(ylabel)) mtext(side=2, line=2, ylabel);
  if ("innerOrOuter" %in% names(attributes(data))) {
    inner_indices <- attr(data, "innerOrOuter") == 1;
    points(data[1, inner_indices], data[2, inner_indices], pch=20, cex=.5, col=rgb(1, 0, 0, .25));
    points(data[1, !inner_indices], data[2, !inner_indices], pch=20, cex=.5, col=rgb(0, 0, 1, .25));
  }
}
```

# Spectral Clustering

This section introduces the spectral clustering algorithm.

## Perfect Projections

The spectral clustering method is based upon perfect projections of input data. These projections might be in a lower dimension than the input data.
The left figure shows input data that is randomly clustered in an inner and an outer circle. The colors are only for visualization purposes and are not known to or used by the algorithm.
The right figure shows the perfect projections calculated by the spectral clustering algorithm when using the Gaussian kernel with a gamma value of 30.


```{r, cache=TRUE, echo=FALSE, fig.asp=.55}
par(mfrow=c(1,2), mar=c(3,3,2,1))

set.seed(1)

X <- clustering::generate_nested_2d_training_data(400)
plot2d(X, title="Input Data",xlabel="", ylabel="")

X_projections <- spectral_clustering(X, 2, gaussian_kernel_with_fixed_gamma(30));
attr(X_projections, "innerOrOuter") <- attr(X, "innerOrOuter");
plot2d(X_projections, title="Projections for gamma = 30",xlabel="", ylabel="")
```

## Retrieving Clusters

The perfect projections alone do not provide a direct clustering themselves, but using one of the other clustering algorithms the spectral clusters can be easily extracted.

sample figure here :)


# Proximity-based Clustering Algorithms
## DBSCAN

The DBSCAN algorithm searches for objects which contain a certain amount of objects $minPts$ in a given radius $\varepsilon$. The radius, called the neighborhood, and the expected number of neighboring objects are specified as arguments.
The left figure demonstrates a neighborhood around a point for a given epsilon (the neighborhood radius) which would satisfy a minimum-points-criterion of four points, since the point itself is counted as well.
The figure to the right shows data that has been clustered using the DBSCAN algorithm.

```{r, echo=FALSE, fig.asp=.55, warning=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

plot(0,0, pch=1, cex=15, main="Example of a Neighborhood")
#points(0,0, pch=1, cex=30)
segments(0, 0, .435, 0, lwd=2)
text(.225, 0.075, "epsilon", cex=.75)
points(0,0, pch=20, cex=1.5, col="blue")
points(c(0.25, -0.2, -.25), c(0.2,.1, -.3), pch=20, cex=1.5, col="darkgreen")
points(c(0.6888805, -0.7046547, -0.4857517, -0.6026919, 0.7602918), c(-0.7698194, -0.5060397, 0.6755757, -0.8241908, 0.4975237), pch=20, cex=1.5, col="darkred")

set.seed(188)
n <- 50
x <- cbind(
  x = runif(5, 0, 2) + rnorm(n, sd = 0.05),
  y = runif(5, 0, 2) + rnorm(n, sd = 0.05)
)
x <- t(x)
d <- c(x, .6, .75, .8, 1.3, 0, 1.6, .6, 1, .9, .9, .5, 1.5, 1, .4, .1, .6);
x <- matrix(d, nrow=2);

clustering::plot_clustered_2d_data(DBSCAN(x, .1, 7));
```

## OPTICS

The OPTICS algorithm does use the same method of determining neighborhoods of points, and, additionally, introduces the concepts of core distance and reachability distance. The core distance is the distance from the center point to the $minPts$th point and is therefore only defined for points with a neighborhood satisfying the minimum points criterion.

```{r, echo=FALSE, fig.asp=.55, warning=FALSE}
par(mfrow=c(1,2), mar=c(3,3,2,1))

plot(0,0, pch=1, cex=15, main="Example of a Core-distance")
#points(0,0, pch=1, cex=30)
segments(0, 0, .435, 0, lwd=2)
text(.225, 0.075, "epsilon", cex=.75)

segments(0, 0, -.25, -.3, lwd=2, col="purple")
text(-.2, -0.125, "core-dist", cex=.75, srt=47.5)

points(0,0, pch=20, cex=1.5, col="blue")
points(c(0.25, -0.2, -.25), c(0.2,.1, -.3), pch=20, cex=1.5, col="darkgreen")
points(c(0.6888805, -0.7046547, -0.4857517, -0.6026919, 0.7602918), c(-0.7698194, -0.5060397, 0.6755757, -0.8241908, 0.4975237), pch=20, cex=1.5, col="darkred")

set.seed(188)
n <- 50
x <- cbind(
  x = runif(5, 0, 2) + rnorm(n, sd = 0.05),
  y = runif(5, 0, 2) + rnorm(n, sd = 0.05)
)
x <- t(x)
d <- c(x, .6, .75, .8, 1.3, 0, 1.6, .6, 1, .9, .9, .5, 1.5, 1, .4, .1, .6);
x <- matrix(d, nrow=2);

clustering::reachability_plot(clustering::OPTICS(x, .1, 7), .074)
```

# K-Medoids Algorithm

The k-medoids algorithm is a clustering approach related to k-means clustering. It is used to partition a data set into k clusters. Each cluster is represented by one medoid. A medoid can be defined as the point in the cluster, whose dissimilarities with all the other points in the cluster is minimum. The k-medoids algorithm requires the user to specify the number of clusters k

## Example data with 2 or 3 clusters

```{r, cache=TRUE, echo=FALSE, fig.asp=.6}
par(mfrow=c(1,2), mar=c(3,3,2,1))
X <- matrix(c(8,7,3,7,4,9,9,6,8,5,5,8,7,3,8,4,7,5,4,5), nrow = 2)

suppressWarnings(clustering::plot_clustered_2d_data(k_medoids(X, 2), show_noise = FALSE, show_legend = TRUE))

suppressWarnings(clustering::plot_clustered_2d_data(k_medoids(X, 3), show_noise = FALSE, show_legend = TRUE))
```
